import spacy
import os
import unicodedata
import re

# Chargement du mod√®le spaCy fran√ßais
try:
    nlp = spacy.load("fr_core_news_sm")
except:
    print("Erreur : le mod√®le spaCy 'fr_core_news_sm' n'est pas install√©.")
    exit()

# Liste personnalis√©e de mots √† supprimer
mots_a_supprimer = {
    "le", "la", "les", "un", "une", "des", "de", "du", "au", "aux", "en",
    "l", "d", "ce", "ces", "cet", "cette", "mon", "ton", "son", "ma", "ta",
    "sa", "mes", "tes", "ses", "nos", "vos", "leurs"
}

def enlever_accents(texte):
    """Remplacer les caract√®res accentu√©s par leurs √©quivalents sans accent"""
    texte = unicodedata.normalize('NFD', texte)
    texte = texte.encode('ascii', 'ignore').decode('utf-8')
    return texte

def charger_fichier(chemin):
    """√âtape 2.1 : Charger un fichier texte et afficher les premi√®res lignes"""
    if not os.path.exists(chemin):
        print(f"Erreur : le fichier '{chemin}' n'existe pas.")
        return None
    
    try:
        with open(chemin, 'r', encoding='utf-8') as f:
            contenu = f.read()
            print("üìÑ Premi√®res lignes du fichier :")
            print("\n".join(contenu.splitlines()[:5]))
            return contenu
    except UnicodeDecodeError:
        print("Erreur : probl√®me d'encodage du fichier.")
        return None
    except Exception as e:
        print(f"Erreur inconnue : {e}")
        return None

def nettoyer_texte(texte):
    """√âtape 2.2 : Nettoyage et pr√©paration du texte"""
    print("\nüîç Texte original (extrait) :")
    print(texte[:300], "...\n")

    # 1. Mise en minuscules
    texte = texte.lower()

    # 2. Remplacement des caract√®res accentu√©s
    texte = enlever_accents(texte)

    # 3. Suppression de tout sauf lettres et espaces
    texte = re.sub(r"[^a-z\s]", " ", texte)

    # 4. Tokenisation
    doc = nlp(texte)

    tokens_nettoyes = []
    for token in doc:
        if (
            not token.is_stop
            and token.text not in mots_a_supprimer
            and not token.is_punct
            and not token.is_space
        ):
            tokens_nettoyes.append(token.lemma_)

    print("‚úÖ Liste finale des mots nettoy√©s :")
    print(tokens_nettoyes)
    return tokens_nettoyes

# üîß Exemple d'utilisation
if __name__ == "__main__":
    chemin_fichier = "mon_texte.txt"  # Remplace ce chemin par ton fichier r√©el
    texte = charger_fichier(chemin_fichier)
    
    if texte:
        nettoyer_texte(texte)
